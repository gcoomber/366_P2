Part 1:

Q1:
The each tiling is a 11 x 11 grid. 11*11 = 121. Therefore each tiling has 121 tiles
starting at index 0 and increasing to 120 (inclusive). The second tiling also contains
121 tiles, but if all of the tiles from all of the tilings are stored in one array the
tiles from tiling 2 start at 121 and increase to 241 (inclusive). This ensures tiling 2
also contains 121 tiles.

Q2:
Each subsequent tiling is shifted in the x and y directions by 1 / numTilings from the 
input space. Therefore, each set of input coordinates (x,y) must be translated into the 
appropriate coordinates in the given tiling. For tiling N (where N ranges from 0 to 7 in
our case), the input coordinates are translated by adding (N * (1 / numTilings)) * tileSize
to each coordinate. ie. (x,y) -> (x + (N * 0.6)/8, y + (N * 0.6)/8).

If in1 = 0.1 and in2 = 0.1, the translated tiling coordinates will be between 0 and 1
for tilings 0 to 6 (since 0.1 + (6 * 0.6)/8 = 0.1 + 0.45 = 0.55). 0.55 is less than 0.6
so the value is still in the first tile of the tiling. Therefore the input point
(0.1,0.1) will be in the first tile of each tiling from tiling 0 to 6. The first tile
of tilings 0 to 7 are 0, 121, 242, 363, 484, 605, and 726. Note that the first tile
of each tiling is NumTilesPerTiling * N where N is the tiling number. Also note that the general
formula for the index of a tile in the 1D array that stores all the tiles for all the 
tilings is InputSpaceTileNum + NumTilesPerTiling * N where N is the index of the tiling.

Q3 + Q4:
If in1 = 0.1 and in2 = 0.1, the coordinates in the 8th tiling are 0.1 + (7*0.6)/8 =  0.1 
+ 0.55 = 0.65. ie. (0.1, 0.1) -> (0.65, 0.65). Since 0.65 is greater than 0.6, the point
is in the second tile of the second row of tiles in the 8th tiling (ie tiling with index
7). If the index of the first tile in this tiling was 0, the index of this tile would be
12 (ie tile 13) (can confirm this by counting the tiles). However, the index of the first 
tile of this tiling is NumTilesPerTiling * N = 121 * 7 =  847. Therefore, in this tiling, the index
is 847 + 12 = 859

Q5:
The maximum tile index is the index of the final tile of the last tiling (tiling with index
7). Given the answer from Q3 and Q4, the first tile of the last tiling is NumTilesPerTiling * N
= 121 * 7 = 847. The last tile of the tiling is 120 indices after the first index (121 tiles
in the tiling). Therefore the last tile of the tiling is 967.

Q6:
The second and fourth example prints should have similar values because the input points
for the second and forth example are very close. (4.0, 2.0) vs (4.0, 2.1). The indicies
are be the same for the two points for all tilings except the tilings with index 4 and
5. The point with 2.1 is 0.1 away from 2.0. The point with 2.1 is shifted over by 0.6/8
each tiling. Therefore, it is classified in a new tile before the point with 2.0 (this
happens at tiling with index 4). They remain in separate tiles for subsequent tiling until
the point with 2.0 catches up and is reclassified into the same tile as the point with 2.1.

Output of Tilecoder.py:
Tile indices for input ( 0.1 , 0.1 ) are :  [0, 121, 242, 363, 484, 605, 726, 859]
Tile indices for input ( 4.0 , 2.0 ) are :  [39, 160, 281, 403, 524, 645, 777, 898]
Tile indices for input ( 5.99 , 5.99 ) are :  [108, 241, 362, 483, 604, 725, 846, 967]
Tile indices for input ( 4.0 , 2.1 ) are :  [39, 160, 281, 403, 535, 656, 777, 898]

Part 2:

Output for 3:
Georges-MacBook-Air:p2 gcoomber$ python3 SuperLearn.py 
Tile indices for input ( 0.1 , 0.1 ) are :  [0, 121, 242, 363, 484, 605, 726, 859]
Tile indices for input ( 4.0 , 2.0 ) are :  [39, 160, 281, 403, 524, 645, 777, 898]
Tile indices for input ( 5.99 , 5.99 ) are :  [108, 241, 362, 483, 604, 725, 846, 967]
Tile indices for input ( 4.0 , 2.1 ) are :  [39, 160, 281, 403, 535, 656, 777, 898]
Tile indices for input ( 4.0 , 2.1 ) are :  [39, 160, 281, 403, 535, 656, 777, 898]
Example ( 0.1 , 0.1 , 2.0 ):     f before learning:  0.0     f after learning :  0.2
Example ( 4.0 , 2.0 , -1.0 ):     f before learning:  0.0     f after learning :  -0.1
Example ( 5.99 , 5.99 , 3.0 ):     f before learning:  0.0     f after learning :  0.3
Example ( 4.0 , 2.1 , -1.0 ):     f before learning:  -0.075     f after learning :  -0.1675

The before value of the fourth point should be nonzero because the input coordinates 
of the fourth point are very close to the input coordinates of the second point (4.0, 2.0)
vs (4.0, 2.1). If we look at the output of part 1, we can see that the two points share
indicies of non-zero features, namely 39, 160, 281, 403, 777, and 898. The second point
updates the weights of these features. The fourth point uses the updated weights to calculate
the before function value. Since the weights are non-zero for before in point 4, the
resultant function value is also non-zero.

Output for 4:
Georges-MacBook-Air:p2 gcoomber$ python3 SuperLearn.py 
Tile indices for input ( 0.1 , 0.1 ) are :  [0, 121, 242, 363, 484, 605, 726, 859]
Tile indices for input ( 4.0 , 2.0 ) are :  [39, 160, 281, 403, 524, 645, 777, 898]
Tile indices for input ( 5.99 , 5.99 ) are :  [108, 241, 362, 483, 604, 725, 846, 967]
Tile indices for input ( 4.0 , 2.1 ) are :  [39, 160, 281, 403, 535, 656, 777, 898]
The estimated MSE:  0.262887693352
The estimated MSE:  0.0550328883111
The estimated MSE:  0.0203256278163
The estimated MSE:  0.0140740020921
The estimated MSE:  0.0121410158657
The estimated MSE:  0.0118655238685
The estimated MSE:  0.0118533617433
The estimated MSE:  0.0113831988978
The estimated MSE:  0.0110173573188
The estimated MSE:  0.0114675495645
The estimated MSE:  0.0112186501035

The estimated MSE does not decrease further towards zero past around 0.01 because the 
generating the target includes a normally distributed random number term with a standard
deviation of 0.1. This means that as we learn and the weights are updated to better approximate
the target, we will never be able to predict the value produced my the random term in the
target. No matter how good the approximation gets, we will always have an error of approximately
0.01. Note that the value 0.01 is because we consider the mean squared error and the square
of 0.1 is 0.01.

6.
The output for the learned function after 20 examples does not look like the plot
of the target function. This is because the relatively small sample count of 20 does not
fully sample the input space. The plot is largely flat at a function value of 0 which is
because there were no samples in these locations of the input space. As a result the
weights for most locations in the sample space were not updated and remain 0. Each sample
exaggerates the importance of the observed data thereby creating peaks and valleys that
dont accurately follow the shape of the target function. 

We have approximately 7 valleys and 4 peaks for a total of 11 peaks and valleys. There
are approximately 4 peaks/valleys that are sharp. This can be explained by the fact that
we did not get even sampling close to these points. This causes the sample importance to
be exaggerated and the resulting value is a sharp peak without smoothing. The smaller peaks
and valleys are by having multiple samples in the same general area (which smooths the graph)
or having low values of the function at these points. 

The width of each peak/valley is approximately 1. This is because few peaks or valleys were
created from multiple samples in the same general area. As a result the peak/valley wasn't 
smoothed and the peak/valley has a width of approximately 1 tile. 

The number of peaks/valleys is approximately 11, which suggests that 9 samples overlapped
with previous samples, thereby creating wider peaks/valleys for those 9 samples. These two
counts added together is on the order of the number of sample of 20.

If the input space was 11x21 with 20 samples, most peaks/valleys would still be created
from one sample. Therefore, the width of the peak would still be close to 1 tile. However,
since tiling is 21 tiles in the in1 direction, the peaks/valleys would be thinner in the 
in1 dimension. 

If we continued to sample past 20 samples, the plot would approach the shape of the target
plot (see the attached pdf). The change in tiling dimensions would not change the final
shape of the learned function after many samples because the learned function would 
approximate the target closely. 
